{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming For Analytics\n",
    "##### © Pratik Agrawal, 2016\n",
    "\n",
    "## Combining Data, Transforming, and Graphing\n",
    "- Combining Data\n",
    " - `shape`\n",
    " - `concat()`\n",
    " - `merge()`\n",
    "- Transforming\n",
    " - `lambda`\n",
    " - `apply()`\n",
    " - `applymap()`\n",
    " - `groupby()`\n",
    "- Graphing \n",
    " - `matplotlib`\n",
    " - `plot()`\n",
    "   - line plot\n",
    "     - plot against indices\n",
    "     - multiple data sets\n",
    "   - launching plot inline/separate window\n",
    "   - plot format strings\n",
    "   - scatter plots\n",
    "   - multiple plot windows\n",
    "   - multiple plots in one window\n",
    "   - legend\n",
    "   - titles and grid\n",
    " - histograms\n",
    " - `seaborn`\n",
    "- Exercises!\n",
    "- Bonus : cleaning column names\n",
    " \n",
    " \n",
    "## Combining Data\n",
    "### 1. `shape`\n",
    "We had a brief look at `dataframes` from `pandas` in the last class. Lets look at how we can find out the number of rows and columns in a given data set. Lets first discover the files `../data/aapl*`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "files_aapl = glob.glob('../programming-for-analytics-course-material/data/aapl*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "files_aapl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets now read each of these files into a list using `pandas read_csv()` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_of_aapl_df = []\n",
    "for i in files_aapl:\n",
    "    j = pd.read_csv(i,infer_datetime_format = True)\n",
    "    list_of_aapl_df.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "?pd.read_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take a look at each of the files that were read in-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list_of_aapl_df[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list_of_aapl_df[1].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list_of_aapl_df[2].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we call `head()` on the variable `list_of_aapl_df` ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list_of_aapl_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have been able to confirm that this is the required data. We should now check the dimensions of this data set- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list_of_aapl_df[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list_of_aapl_df[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list_of_aapl_df[2].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. `concat()`\n",
    "In the last assignment you had to write a lot of code to probably concatenate the three files together. Lets see a simpler way to concatenate all the data frames- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "aapl_df = pd.concat(list_of_aapl_df)\n",
    "aapl_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets confirm that all the data was concatenated-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "aapl_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we had read in the files into separate variables-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_aapl_1 = list_of_aapl_df[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_aapl_2 = list_of_aapl_df[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_aapl_3 = list_of_aapl_df[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to concatenate all these files. In order to do this you must remember that `concat()` function accepts a list of variables only, and not individual variables-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "aapl_df = pd.concat(df_aapl_1, df_aapl_2, df_aapl_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "aapl_df = pd.concat([df_aapl_1, df_aapl_2, df_aapl_3])\n",
    "aapl_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "aapl_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. `merge()`\n",
    "Up until now we have seen how to concatenate data along an axis. What if we have to join the files on an variable/column.\n",
    "\n",
    "Lets read in the Divvy data set files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_trips=pd.read_csv(\"../programming-for-analytics-course-material/data/Divvy_Trips_2013.csv\", infer_datetime_format=True)\n",
    "df_stations=pd.read_csv(\"../programming-for-analytics-course-material/data/Divvy_Stations_2013.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets take a quick peek at the head for each data frame\n",
    "*Remember that the head function has a default n=5. Which implies that it displays only the first 5 rows of the dataframe*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_trips.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_stations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cols_trips = list(df_trips.columns)\n",
    "cols_trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cols_stations = list(df_stations.columns)\n",
    "cols_stations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Join Number 1\n",
    "Both *from* and *to* station ids in the trips dataset have static data in the second table (stations). In order to run calculations and google maps api queries etc. we need to join first for the *from_station_id* and second for the *to_station_id*\n",
    "\n",
    "##### First lets check the shape of the trips dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_trips.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now lets run the merge/join operation, and store the result in a new df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_from=pd.merge(df_trips,df_stations,left_on=\"from_station_id\",right_on=\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "?pd.merge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Lets check the shape again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_from.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_from.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Join Number 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_divvy=pd.merge(df_from,df_stations,left_on=\"to_station_id\",right_on=\"id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Lets check the shape again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_divvy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_divvy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list(df_divvy.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note that *pandas* automatically added the suffix \\_x and \\_y to the column names in conflict. In our example \\_x represents details for the *from_station* and \\_y represents details for the *to_station* \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming\n",
    "\n",
    "More often than not you will need to clean/transform your data. Running `for` loops is not going to be efficient. We will look at various functions that allow us to clean and transform the data quickly. \n",
    "\n",
    "### 1. `lambda`\n",
    "\n",
    "Q. What is a lambda?\n",
    "\n",
    "A. the 11th letter of the Greek alphabet\n",
    "\n",
    "B. the craniometric point at the junction of the sagittal and lamboid sutures of the skull\n",
    "\n",
    "C. the name of a series of Japanese rocket\n",
    "\n",
    "D. anonymous (unbound) functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    " \n",
    "def sqroot(x):\n",
    "    \"\"\"\n",
    "    Finds the square root of the number passed in\n",
    "    \"\"\"\n",
    "    return math.sqrt(x)\n",
    " \n",
    "square_rt = lambda x: math.sqrt(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sqroot(49)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "square_rt(49)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets read in the sandwiches data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_best_sws = pd.read_csv('../programming-for-analytics-course-material/data/best-sandwiches.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_best_sws.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. `apply()`\n",
    "\n",
    "Applies function along input axis of DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_best_sws.price.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets get rid of the `$` sign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_best_sws['price'] = df_best_sws['price'].apply(lambda x: x.strip('$'))\n",
    "df_best_sws.price.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you notice there are still `.` symbols left in the data. Before we can use this price column for any calculations, we need to clean it up thoroughly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_best_sws['price'] = df_best_sws['price'].apply(lambda x: x.strip('.'))\n",
    "df_best_sws.price.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. `applymap()`\n",
    "\n",
    "Apply a function to a DataFrame that is intended to operate elementwise, i.e. like doing map(func, series) for each series in the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols = ['description', 'address', 'sandwich','restaurant','full_address','formatted_address']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_best_sws[cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we need to set filters for different pieces of text, our scripts will get thrown off when they encounter mixed case characters. So lets convert all text in the columns to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_best_sws[cols] = df_best_sws[cols].applymap(lambda x: x.lower())\n",
    "df_best_sws.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. `groupby()`\n",
    "\n",
    "refers to a process involving one or more of the following steps\n",
    "\n",
    "__*Splitting*__ the data into groups based on some criteria\n",
    "__*Applying*__ a function to each group independently\n",
    "__*Combining*__ the results into a data structure\n",
    "\n",
    "Of these, the split step is the most straightforward. In fact, in many situations you may wish to split the data set into groups and do something with those groups yourself. In the apply step, we might wish to one of the following:\n",
    "\n",
    "__*Aggregation*__: computing a summary statistic (or statistics) about each group. Some examples:\n",
    "\n",
    "- Compute group sums or means\n",
    "- Compute group sizes / counts\n",
    "\n",
    "__*Transformation*__: perform some group-specific computations and return a like-indexed. Some examples:\n",
    "\n",
    "- Standardizing data (zscore) within group\n",
    "- Filling NAs within groups with a value derived from each group\n",
    "\n",
    "__*Filtration*__: discard some groups, according to a group-wise computation that evaluates True or False. Some examples:\n",
    "\n",
    "- Discarding data that belongs to groups with only a few members\n",
    "- Filtering out data based on the group sum or mean\n",
    "\n",
    "Some combination of the above: GroupBy will examine the results of the apply step and try to return a sensibly combined result if it doesn’t fit into either of the above two categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "df_best_sws[df_best_sws.price==7.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_best_sws.groupby(\"price\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def temp_func(mini_df):\n",
    "    return pd.Series({\"average_rank\":np.mean(mini_df[\"rank\"])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_best_sws.groupby(\"price\").apply(temp_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_best_sws[\"price\"] = df_best_sws.price.convert_objects(convert_numeric=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_best_sws.price.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%pylab inline\n",
    "df_best_sws.price.hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphing\n",
    "### 1. `matplotlib`\n",
    "`matplotlib` provides tons of tools for creating line plots, image plots, and even some 3D plots. When you write scripts using these tools, you will need to import the desired functions.  There are a couple of common approaches.  The first is to import the specific functions you want using:\n",
    "    \n",
    "    from matplotlib.pyplot import plot, subplot, figure, etc.\n",
    "\n",
    "An alternative that is also used quite often is:\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "Using this approach, you can refer to the plot commands through the `plt` name such as `plt.plot`.\n",
    "\n",
    "\n",
    "### 2. `plot()`\n",
    "\n",
    "The **`plot`** command is the swiss army knife of plotting commands.  It defaults to creating line plots, but it is possible to control many different aspects of data display (marker types, color, transparency, etc.) through its key word arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### line plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### plot against indices\n",
    "\n",
    "When calling **`plot`** with a single argument (in this case `sin(x)`), the values of the array are used as the y values, and the indices of the array (0, 1, 2, ...) are used as the x axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = np.linspace(0, 2*np.pi, 50)\n",
    "plt.plot(np.sin(x))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When calling plot with two arguments, the first is used as the x value, and the second is used as the y value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot(x,sin(x))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### multiple data sets\n",
    "\n",
    "Calling **`plot`** with multiple argument pairs results in multiple lines within a plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot(x, sin(x), x, sin(2*x))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. launching `plot()` inline/separate window\n",
    "It is possible, however, to launch an external plot window from using the magic command **`%pylab qt`**. There are multiple benefits to external windows including much more interactive plots (zooming, panning, etc.).  You can use this window to experiment with the various toolbars. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%pylab qt\n",
    "plot(x, sin(x))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "To switch back to inline plots, use the magic command **`%pylab inline`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "plot(x, sin(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. plot format strings\n",
    "\n",
    "If and x,y data pair is followed by a string, it is interpreted as a format string for the line that allows you to specify color, line type, and marker type.  To see a full list of options for format strings, type **`plot?`** at the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "plot(x, sin(x), 'r-^')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot(x, sin(x), 'b-o', x, sin(2*x), 'r-^')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. scatter plots\n",
    "\n",
    "The **`scatter`** function can create simple x, y scatter plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x = np.linspace(0, 2*pi, 50)\n",
    "y = sin(x)\n",
    "scatter(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = randn(200)\n",
    "y = randn(200)\n",
    "size = rand(200) * 30\n",
    "color = rand(200)\n",
    "scatter(x, y, size, color)\n",
    "colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. multiple plot windows\n",
    "\n",
    "The **`figure`** function will create new plot windows (when not in the inline plot display mode in notebooks).  In notebooks, it simply creates two separate plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%pylab qt\n",
    "t = np.linspace(0, 2*pi, 50)\n",
    "x = sin(t)\n",
    "y = cos(t)\n",
    "\n",
    "# create the first window\n",
    "figure()\n",
    "plot(x)\n",
    "plt.show()\n",
    "\n",
    "# create the second window.\n",
    "figure()\n",
    "plot(y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. multiple plots in one window\n",
    "\n",
    "The **`subplot`** function allows you to position two separate plots within a single plot window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "x = array([1,2,3,2,1])\n",
    "y = array([1,3,2,3,1])\n",
    "\n",
    "subplot(3,1,1)\n",
    "plot(x)\n",
    "\n",
    "subplot(3,1,2)\n",
    "plot(y)\n",
    "\n",
    "subplot(3,1,3)\n",
    "plot(x)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. `legend`\n",
    "\n",
    "The **`legend`** function will add a legend to plots, using the specified `label` keyword argument that was passed into each plot function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot(sin(x), label='sin')\n",
    "plot(cos(x), label='cos')\n",
    "legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example where we have three lines, but have only labeled two.  **`legend`** is smart enough to only put the labeled lines into the legend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot(sin(x), label='sin')\n",
    "plot(sin(2*x))\n",
    "plot(cos(x), label='cos')\n",
    "legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the plot functions have not assigned a label to the lines, then you can pass a list of labels to the **`legend`**.  The labels are assigned to the data sets in the same order they were added to the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot(sin(x))\n",
    "plot(cos(x))\n",
    "legend(['sin', 'cos'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. titles and grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot(x, sin(x))\n",
    "xlabel('radians')\n",
    "ylabel('amplitude', fontsize='large')\n",
    "title('Sin(x)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. histograms\n",
    "\n",
    "**`hist`** will create a histogram with 10 bins by default.  This can be changed by specifying the `bins` keyword argument.\n",
    "\n",
    "Note that matplotlib's `hist` function can be slow on large data sets.  In these cases, it can be beneficial to use the `scipy.stats.histogram` method which is faster.  However, it only does the calculations. It'll be up to you to make the plot from the returned values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "seed(13)\n",
    "hist(randn(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hist(randn(1000))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hist(randn(1000), 30)\n",
    "hist(randn(100), 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. `seaborn`\n",
    "\n",
    "The `seaborn` library can make your graphs look better. You can compare this to `ggplot`\n",
    "\n",
    "To install this library run- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hist(randn(1000))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hist(randn(1000), 30)\n",
    "hist(randn(100), 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises!\n",
    "\n",
    "Run the following line before you start creating plots. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aapl_df[\"Date\"] = pd.to_datetime(aapl_df.Date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will convert the Date column of your data frame into a date time object, which allows `pandas` and `python` to provide \n",
    "- chronological ordering\n",
    "- derived variables such as week of year, weekday, isweekday, isweekend, day of week etc.\n",
    "- this also allows you to fit splines/interpolate data\n",
    "\n",
    "### Q1.\n",
    "We combined all the data for `aapl` in the first section. Lets try to plot a few columns. \n",
    "- plot `Date` v/s `Adj Close`\n",
    "- plot `Date` v/s `High` & `Low` (in one graph, with different colors)\n",
    "- plot `Date` v/s difference between `High` & `Low`\n",
    "\n",
    "### Q2.\n",
    "Is there a differnce in the trip times between `Customers` and `Subscribers` in the Divvy Dataset?\n",
    "- Provided descriptive statistics for each segment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solutions\n",
    "### Q1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot(aapl_df.Date,aapl_df[\"Adj Close\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot(aapl_df.Date,aapl_df.High,'b')\n",
    "plot(aapl_df.Date,aapl_df.Low,'r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot(aapl_df.Date,(aapl_df.High-aapl_df.Low))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def descritive_statistics(mini_df):\n",
    "    return mini_df.describe()\n",
    "\n",
    "df_divvy.groupby(\"usertype\").apply(descritive_statistics)[\"tripduration\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_divvy.usertype.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus\n",
    "### cleaning column names\n",
    "\n",
    "Why?\n",
    "\n",
    "To avoid mismatch between joins and concatenates between different tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['trip_id',\n",
       " 'starttime',\n",
       " 'stoptime',\n",
       " 'bikeid',\n",
       " 'tripduration',\n",
       " 'from_station_id',\n",
       " 'from_station_name',\n",
       " 'to_station_id',\n",
       " 'to_station_name',\n",
       " 'usertype',\n",
       " 'gender',\n",
       " 'birthday']"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df_trips.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['TRIP_ID',\n",
       " 'STARTTIME',\n",
       " 'STOPTIME',\n",
       " 'BIKEID',\n",
       " 'TRIPDURATION',\n",
       " 'FROM_STATION_ID',\n",
       " 'FROM_STATION_NAME',\n",
       " 'TO_STATION_ID',\n",
       " 'TO_STATION_NAME',\n",
       " 'USERTYPE',\n",
       " 'GENDER',\n",
       " 'BIRTHDAY']"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_trips.columns=[c.upper() for c in df_trips.columns]\n",
    "list(df_trips.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id',\n",
       " 'name',\n",
       " 'latitude',\n",
       " 'longitude',\n",
       " 'dpcapacity',\n",
       " 'landmark',\n",
       " 'online date']"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df_stations.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ID',\n",
       " 'NAME',\n",
       " 'LATITUDE',\n",
       " 'LONGITUDE',\n",
       " 'DPCAPACITY',\n",
       " 'LANDMARK',\n",
       " 'ONLINE DATE']"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stations.columns=[c.upper() for c in df_stations.columns]\n",
    "list(df_stations.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### renaming columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['TRIP_ID',\n",
       " 'START_TIME',\n",
       " 'STOP_TIME',\n",
       " 'BIKE_ID',\n",
       " 'TRIP_DURATION',\n",
       " 'FROM_STATION_ID',\n",
       " 'FROM_STATION_NAME',\n",
       " 'TO_STATION_ID',\n",
       " 'TO_STATION_NAME',\n",
       " 'USER_TYPE',\n",
       " 'GENDER',\n",
       " 'BIRTHDAY']"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_trips = df_trips.rename(columns={'TRIPDURATION': 'TRIP_DURATION', \n",
    "                        'BIKEID': 'BIKE_ID',\n",
    "                        'STARTTIME': 'START_TIME',\n",
    "                        'STOPTIME': 'STOP_TIME',\n",
    "                        'USERTYPE': 'USER_TYPE'})\n",
    "list(df_trips.columns)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
